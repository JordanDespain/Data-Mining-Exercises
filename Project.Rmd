---
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, warnings = NA)

library(tidyverse)
library(modelr)
library(rsample)
library(mosaic)
library(caret)
library(R.utils)
library(foreach)
library(ggpubr)
library(corrplot)

options(scipen = 999)


```

```{r, include=FALSE}
setwd("C:/Users/Despa/Desktop/UT AUSTIN MASTERS CLASSES/ECO 395M Data Mining and Statistical Learning")

dir()
```

\begin{center}
\begingroup
\fontfamily{cmr}\fontsize{14}{16}\selectfont
To Win A Cy Young
\endgroup

\begingroup
\fontfamily{cmr}\fontsize{10}{12}\selectfont
Jordan Despain and Jack Freeman
\endgroup

\begingroup
\fontfamily{cmr}\fontsize{10}{12}\selectfont
04/24/2023
\endgroup
\end{center}

&nbsp;

\begingroup
\fontfamily{cmr}\fontsize{14}{16}\selectfont
**Bob Lyur: The Best Pitching Coach To Ever Live**
\endgroup  


(Note: Our backstory is fiction and is meant for entertainment)

&nbsp;

\begingroup
\fontfamily{cmr}\fontsize{12}{12}\selectfont
**The Story**
\endgroup  

``` {r, include = FALSE, warning = FALSE, message = FALSE}

# COMMENTED OUT CODE USED TO CLEAN DATA IN CASE INTERESTED

# y1956 <- read.csv("https://raw.githubusercontent.com/JordanDespain/Data-Mining-Exercises/main/Project-Data/Raw-Individual-Years/1956.csv")
# 
# y1956$Year <- 1956
# 
# df_all <- data.frame(matrix(ncol = 37, nrow = 0))
# 
# colnames(df_all) <- colnames(y1956)
# 
# 
# 
# for (y in 1956:2022) {
#   data <- read.csv(paste('https://raw.githubusercontent.com/JordanDespain/Data-Mining-Exercises/main/Project-Data/Raw-Individual-Years/',y,'.csv', sep = ""))
# 
#   data$Year <- y
#   data$Name <- gsub("[[:punct:]]", "", data$Name)
#   names <- unique(data[c("Name.additional")])
# 
#   df <- data.frame(matrix(ncol = 37, nrow = 0))
# 
#   colnames(df) <- colnames(y1956)
# 
#   for (x in 1:nrow(names)) {
#   ifelse(count(data %>% filter(Name.additional == names[x,1]))[[1]] == 1,
#          df[x,] <- data %>% filter(Name.additional == names[x,1]),
#          df[x,] <- data %>% filter(Name.additional == names[x,1] & Tm == "TOT"))
# 
#   }
# 
#   df_all <- rbind(df_all, df)
# 
# }
# 
# all_years <- df_all %>%
#   select(-c("Rk", "W.L.", "ERA.", "SO.W"))
# 
# all_years
# 
# write.csv(all_years, "C:/Users/Despa/Desktop/UT AUSTIN MASTERS CLASSES/ECO 395M Data Mining and Statistical Learning/Data-Mining-Exercises/Project-Data/All_Years.csv")



```

``` {r, include = FALSE, warning = FALSE, message = FALSE}

# COMMENTED OUT CODE USED TO CLEAN DATA IN CASE INTERESTED

# pitchers <- read.csv("https://raw.githubusercontent.com/JordanDespain/Data-Mining-Exercises/main/Project-Data/All_Years.csv")
# 
# 
# cy_young_years <- c(1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1967,
#                     1968, 1968, 1969, 1969, 1969, 1970, 1970, 1971, 1971, 1972, 1972, 1973, 1973,
#                     1974, 1974, 1975, 1975, 1976, 1976, 1977, 1977, 1978, 1978, 1979, 1979, 1980, 
#                     1980, 1981, 1981, 1982, 1982, 1983, 1983, 1984, 1984, 1985, 1985, 1986, 1986, 
#                     1987, 1987, 1988, 1988, 1989, 1989, 1990, 1990, 1991, 1991, 1992, 1992, 1993, 
#                     1993, 1994, 1994, 1995, 1995, 1996, 1996, 1997, 1997, 1998, 1998, 1999, 1999, 
#                     2000, 2000, 2001, 2001, 2002, 2002, 2003, 2003, 2004, 2004, 2005, 2005, 2006, 
#                     2006, 2007, 2007, 2008, 2008, 2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 
#                     2013, 2013, 2014, 2014, 2015, 2015, 2016, 2016, 2017, 2017, 2018, 2018, 2019, 
#                     2019, 2020, 2020, 2021, 2021, 2022, 2022)
# 
# cy_young_winners <- c("Don Newcombe", "Warren Spahn", "Bob Turley", "Early Wynn", "Vern Law", 
#                       "Whitey Ford", "Don Drysdale", "Sandy Koufax", "Dean Chance", "Sandy Koufax",
#                       "Sandy Koufax", "Jim Lonborg", "Mike McCormick", "Denny McLain", "Bob Gibson",
#                       "Tom Seaver", "Mike Cuellar", "Denny McLain", "Jim Perry", "Bob Gibson", 
#                       "Fergie Jenkins", "Vida Blue", "Steve Carlton", "Gaylord Perry", "Jim Palmer",
#                       "Tom Seaver", "Catfish Hunter", "Mike Marshall", "Jim Palmer", "Tom Seaver",
#                       "Jim Palmer", "Randy Jones", "Sparky Lyle", "Steve Carlton", "Ron Guidry",
#                       "Gaylord Perry", "Mike Flanagan", "Bruce Sutter", "Steve Stone", 
#                       "Steve Carlton", "Rollie Fingers", "Fernando Valenzuela", "Pete Vuckovich",
#                       "Steve Carlton", "LaMarr Hoyt", "John Denny", "Willie Hernandez", 
#                       "Rick Sutcliffe", "Bret Saberhagen", "Dwight Gooden", "Roger Clemens",
#                       "Mike Scott", "Roger Clemens", "Steve Bedrosian", "Frank Viola",
#                       "Orel Hershiser", "Bret Saberhagen", "Mark Davis", "Bob Welch", "Doug Drabek",
#                       "Roger Clemens", "Tom Glavine", "Dennis Eckersley", "Greg Maddux",
#                       "Jack McDowell", "Greg Maddux", "David Cone", "Greg Maddux", "Randy Johnson",
#                       "Greg Maddux", "Pat Hentgen", "John Smoltz", "Roger Clemens", "Pedro Martinez",
#                       "Roger Clemens", "Tom Glavine", "Pedro Martinez", "Randy Johnson",
#                       "Pedro Martinez", "Randy Johnson", "Roger Clemens", "Randy Johnson",
#                       "Barry Zito", "Randy Johnson", "Roy Halladay", "Eric Gagne", "Johan Santana",
#                       "Roger Clemens", "Bartolo Colon", "Chris Carpenter", "Johan Santana",
#                       "Brandon Webb", "CC Sabathia", "Jake Peavy", "Cliff Lee", "Tim Lincecum",
#                       "Zack Greinke", "Tim Lincecum", "F\xc3lix Hern\xc3ndez", "Roy Halladay",
#                       "Justin Verlander", "Clayton Kershaw", "David Price", "RA Dickey",
#                       "Max Scherzer", "Clayton Kershaw", "Corey Kluber", "Clayton Kershaw",
#                       "Dallas Keuchel", "Jake Arrieta", "Rick Porcello", "Max Scherzer", 
#                       "Corey Kluber", "Max Scherzer", "Blake Snell", "Jacob deGrom", 
#                       "Justin Verlander", "Jacob deGrom", "Shane Bieber", "Trevor Bauer",
#                       "Robbie Ray", "Corbin Burnes", "Justin Verlander", "Sandy Alcantara")
# 
# 
# cy_youngs <- data.frame(cy_young_years, cy_young_winners)
# 
# pitchers$cy_young <- 0
# 
# for (y in 1:nrow(cy_youngs)) {
#   
#   index <- which(pitchers$Year == cy_youngs[y,1] & pitchers$Name == cy_youngs[y,2])
#   pitchers$cy_young[index] <- 1
#   
#   }
# 
# 
# write.csv(pitchers, "C:/Users/Despa/Desktop/UT AUSTIN MASTERS CLASSES/ECO 395M Data Mining and Statistical Learning/Data-Mining-Exercises/Project-Data/All_Years_With_Cy_Young_Added.csv")
  

```

``` {r, include = FALSE, warning = FALSE, message = FALSE}

model_data <- read.csv("https://raw.githubusercontent.com/JordanDespain/Data-Mining-Exercises/main/Project-Data/All_Years_With_Cy_Young_Added.csv")
model_data <- model_data[,-1]
model_data <- drop_na(model_data)
model_data <- model_data %>% select(-c(Name.additional))


corr_data <- model_data %>%
  select(-c(Name, Tm, Lg, Year))
corrplot(cor(corr_data), method = 'color')



errors <- list()

for (i in 1:10) {

  split <- initial_split(model_data %>% 
                           select(-c(Name)), prop = 0.8)
  train <- training(split)
  test <- testing(split)

  model <- glm(cy_young ~ . - Tm - Lg - Year - GF - GS - IBB - BK - BF, 
               data = train, family = "binomial")
  
  predictions <- predict(model, newdata = test, type = "response")

  errors[[i]] <- RMSE(predictions, test$cy_young)

}

rmse <- round(mean(unlist(errors)), 5)


model <- glm(cy_young ~ . - Tm - Lg - Year - GF - GS - IBB - BK - BF, 
               data = model_data %>%
                           select(-c(Name)), family = "binomial")
  
model_data$predicted_probs <- predict(model, newdata = model_data, type = "response")

above_10_and_won <- model_data %>% filter(cy_young == 1 & predicted_probs > 0.10)
above_10_and_lost <- model_data %>% filter(cy_young == 0 & predicted_probs > 0.10)

avg_probs <- mean(model_data$predicted_probs)


full_hist <- ggplot(model_data, aes(x = predicted_probs)) +
  geom_histogram(color = "black", fill = "white") +
  labs(x = "Predicted Probability of Winning a Cy Young Award", y = "Count", title = "Overall Picture")

zoomed_hist <- ggplot(model_data %>% filter(predicted_probs > 0.005), aes(x = predicted_probs)) +
  geom_histogram(color = "black", fill = "white") +
  labs(x = "Predicted Probability of Winning a Cy Young Award", y = "Count", title = "Zoomed-In Picture")

```


```{r, include = FALSE, message = FALSE, warning = FALSE}

Name <- c("Elijah Green", "Druw Jones", "Termarr Johnson", "Cam Collier", "Dylan Lesko")
Age <- c(14, 15, 15, 14, 14)
Tm <- c("DET", "DET", "DET", "DET", "DET")
Lg <- c("AL", "AL", "AL", "AL", "AL")
W <- c(15, 13, 14, 14, 17)
L <- c(8, 11, 10, 9, 6)
ERA <- c(3.64, 4.13, 4.01, 3.54, 3.11)
G <- c(33, 33, 32, 32, 32)
GS <- c(33, 33, 32, 32, 32)
GF <- c(1, 0, 0, 0, 2)
CG <- c(1, 0, 0, 0, 2)
SHO <- c(0, 0, 0, 0, 1)
SV <- c(0, 0, 0, 0, 0)
IP <- c(175.3, 184.2, 180.7, 180.6, 191.4)
H <- c(197, 221, 208, 219, 188)
R <- c(84, 90, 88, 76, 70)
ER <- c(80, 88, 81, 75, 66)
HR <- c(25, 29, 37, 30, 26)
BB <- c(72, 75, 68, 59, 57)
IBB <- c(3, 2, 1, 0, 1)
SO <- c(163, 145, 132, 170, 190)
HBP <- c(9, 11, 14, 9, 8)
BK <- c(0, 0, 1, 0, 1)
WP <- c(14, 11, 15, 8, 9)
BF <- c(802, 754, 819, 794, 765)

high_schoolers <- data.frame(Name, Age, W, L, ERA, G, GS, GF, CG, SHO, SV, IP, H, R,
                             ER, HR, BB, IBB, SO, HBP, BK, WP, BF)

high_schoolers <- high_schoolers %>%
  mutate(FIP = ((13*HR + 3*(BB+HBP) - 2*SO)/IP + 2.06892), 
         WHIP = (BB + H)/IP,
         H9 = 9*H/IP,
         HR9 = 9*HR/IP,
         BB9 = 9*BB/IP,
         SO9 = 9*SO/IP,
         `SO/W` = SO/W,
         Year = 2022,
         cy_young = 0)


high_schoolers$predicted_probs <- predict(model, newdata = high_schoolers, type = "response")




high_schoolers_year_1_goals <- high_schoolers %>%
  mutate(W = W + 1,
         L = L - 1,
         ERA = ERA*.95,
         H = H*.95,
         R = R*.95,
         ER = ER*.95,
         HR = HR*.95,
         SO = SO*1.05,
         BB = BB*.95,
         HBP = HBP -1,
         WP = WP -1,
         Year = Year + 1)

high_schoolers_year_1_goals <- high_schoolers_year_1_goals %>%
  mutate(FIP = ((13*HR + 3*(BB+HBP) - 2*SO)/IP + 2.06892), 
         WHIP = (BB + H)/IP,
         H9 = 9*H/IP,
         HR9 = 9*HR/IP,
         BB9 = 9*BB/IP,
         SO9 = 9*SO/IP,
         `SO/W` = SO/W)

high_schoolers_year_1_goals$predicted_probs <- predict(model, newdata = high_schoolers_year_1_goals,
                                                    type = "response")



high_schoolers_year_2_goals <- high_schoolers_year_1_goals %>%
  mutate(W = W + 1,
         L = L - 1,
         ERA = ERA*.90,
         H = H*.95,
         R = R*.95,
         ER = ER*.90,
         HR = HR*.90,
         SO = SO*1.1,
         BB = BB*.90,
         HBP = HBP -1,
         WP = WP -1,
         Year = Year + 1)

high_schoolers_year_2_goals <- high_schoolers_year_2_goals %>%
  mutate(FIP = ((13*HR + 3*(BB+HBP) - 2*SO)/IP + 2.06892), 
         WHIP = (BB + H)/IP,
         H9 = 9*H/IP,
         HR9 = 9*HR/IP,
         BB9 = 9*BB/IP,
         SO9 = 9*SO/IP,
         `SO/W` = SO/W)

high_schoolers_year_2_goals$predicted_probs <- predict(model, newdata = high_schoolers_year_2_goals,
                                                    type = "response")



high_schoolers_year_3_goals <- high_schoolers_year_2_goals %>%
  mutate(W = W + 2,
         L = L - 1,
         ERA = ERA*.85,
         H = H*.95,
         R = R*.95,
         ER = ER*.90,
         HR = HR*.90,
         SO = SO*1.1,
         BB = BB*.95,
         HBP = HBP -1,
         WP = WP -2,
         Year = Year + 1)

high_schoolers_year_3_goals <- high_schoolers_year_3_goals %>%
  mutate(FIP = ((13*HR + 3*(BB+HBP) - 2*SO)/IP + 2.06892), 
         WHIP = (BB + H)/IP,
         H9 = 9*H/IP,
         HR9 = 9*HR/IP,
         BB9 = 9*BB/IP,
         SO9 = 9*SO/IP,
         `SO/W` = SO/W)

high_schoolers_year_3_goals$predicted_probs <- predict(model, newdata = high_schoolers_year_3_goals,
                                                    type = "response")



high_schoolers_year_4_goals <- high_schoolers_year_3_goals %>%
  mutate(W = W + 2,
         L = L - 1,
         ERA = ERA*.80,
         H = H*.90,
         R = R*.90,
         ER = ER*.90,
         HR = HR*.90,
         SO = SO*1.1,
         BB = BB*.90,
         HBP = HBP -2,
         WP = WP -2,
         Year = Year + 1)

high_schoolers_year_4_goals <- high_schoolers_year_4_goals %>%
  mutate(FIP = ((13*HR + 3*(BB+HBP) - 2*SO)/IP + 2.06892), 
         WHIP = (BB + H)/IP,
         H9 = 9*H/IP,
         HR9 = 9*HR/IP,
         BB9 = 9*BB/IP,
         SO9 = 9*SO/IP,
         `SO/W` = SO/W)

high_schoolers_year_4_goals$predicted_probs <- predict(model, newdata = high_schoolers_year_4_goals,
                                                    type = "response")


improvement_plot <- ggplot() +
  geom_point(data = high_schoolers,
             aes(x = Year, y = predicted_probs, color = Name, shape = Name), size = 4, alpha = 0.8) +
  geom_point(data = high_schoolers_year_1_goals,
             aes(x = Year, y = predicted_probs, color = Name, shape = Name), size = 4, alpha = 0.8) +
  geom_point(data = high_schoolers_year_2_goals,
             aes(x = Year, y = predicted_probs, color = Name, shape = Name), size = 4, alpha = 0.8) +
  geom_point(data = high_schoolers_year_3_goals,
             aes(x = Year, y = predicted_probs, color = Name, shape = Name), size = 4, alpha = 0.8) +
  geom_point(data = high_schoolers_year_4_goals,
             aes(x = Year, y = predicted_probs, color = Name, shape = Name), size = 4, alpha = 0.8)


```


A potential tragedy is brewing over at Best Baseball High School, which is the best high school in the world when it comes to the sport of baseball. This high school is known for the fantastic players it produces, who more often than not go on to the major leagues and accomplish great things. The pitching coach for the baseball team of the last 45 years had to retire due to health concerns, and the incoming group of freshmen pitchers need someone new to help guide them to greatness. The team has managed to find an amazing replacement, or so they think. Bob Lyur has been hired to come in and take over the pitching program at Best Baseball High School. The group of incoming freshmen have incredible potential and they need someone who is able to unlock this potential and push them as far as possible in the four years they will be with the program. Just looking at some of their stats coming into the program makes it seem like a doable task for a good coach,  

```{r, include=TRUE, warning=FALSE, message=FALSE}

high_schoolers[,-c(6,8,9,11,18,20:33)]


```

&nbsp; 

Bob Lyur seems like the perfect coach to guide these young pitchers as he has a very impressive track record and resume. The only problem is, he has lied about all his accomplishments and knows very little about what it takes to make these freshmen great pitchers. He has never been a coach for anything as a matter of fact. This is a big, big problem. The school wants things to carry on as they always have with this new coach. Every group of freshmen that have come in past have had at least one pitcher reach a goal of "Cy Young ready" by the time they graduate high school. "Cy Young ready" meaning, their stats are good enough to be a serious candidate for the Cy Young award. This goal has been a possible goal ever since high school baseball switched to 162 game seasons 15 years ago, the same number as the major league. This has made it possible to better compare stats from these high school prospects to those in the majors. Bob Lyur knows absolutely nothing about what it takes to win the Cy Young.  

&nbsp;  

Bob soon realizes the mess he is inevitably going to create because there is no way he can get a single pitcher anywhere near this goal that the school is expecting to be accomplished. Bob Lyur is in panic mode trying to find a way he can make it out of this situation without being ousted as the fraud he is. He has come up with a plan. He knows two great data scientists who love baseball. Data is used to manipulate the sport in many different ways nowadays, so he believes surely it can help him get out of this pickle. Bob Lyur calls up these two data scientists and they agree to help, for a price of course. They believe this is a tall task, but are confident they can come up with a plan.  

\pagebreak  

\begingroup
\fontfamily{cmr}\fontsize{12}{12}\selectfont
**The Data**
\endgroup 

The two data scientists are tasked with coming up with a plan for this coach to improve the freshmen pitchers to "Cy Young ready" by the end of their high school career. They first need to collect data and try and figure out which stats are associated with improving the chances of winning a Cy Young award. They know the perfect place to look for this data, baseball-reference.com. The website baseball-reference keeps stats for just about everything in the world of baseball. They were able to get the stats from all pitchers who at least pitched one inning from 1956, when the Cy Young award was first given out, to 2022. The data had to be downloaded by each individual year, so part of their cleaning process was combining each year into one big data set. They also found all the Cy Young award winners throughout history and used this information to create a dummy variable of whether an individual won the Cy Young award for that given year. They believe their data is now ready to be used in a model. Before deciding on a model, the two scientists are curious about how the different variables included in the set are correlated with one another. A simple correlation plot gives them a great idea of each relationship amongst the variables.  

```{r, include=TRUE, warning=FALSE, message=FALSE}

corrplot(cor(corr_data), method = 'color', tl.col = 'black', tl.cex = .75)


```

The data scientists find a lot of these relationships make perfect sense, like hits and innings pitched having a very strong positive correlation. The more innings you pitch, the more hits you're likely to give up because you are facing more batters. However, they do find some of these very interesting. For example, home runs given up per 9 innings, HR9, having a negative relationship with home runs given up. You would think it would have a strong positive correlation given the number of home runs is in the numerator of the formula for the stat HR9. They believe this may have something to do with if you're giving up more home runs you may be pitching more innings, and innings pitched is the denominator in the formula. Nonetheless, the data scientists must remain focused on their task. So they start focusing on the model the want to build to hopefully save the legacy of Best Baseball High School and the reputation of Bob Lyur.  

\pagebreak

\begingroup
\fontfamily{cmr}\fontsize{12}{12}\selectfont
**The Model**
\endgroup

While considering which model is best fit for this data, the data scientists remember a time they recently used a logistic model. After a little discussion, they believe a logistic model is perfect for their task, because they want to predict *cy_young*, which is a dummy variable that only takes on the two values, 0 and 1. Also, this will allow them to determine a percent chance of winning the Cy Young award, which is exactly what is needed to make a claim that someone is "Cy Young ready". Since they are experts with the programming language R, they will use the glm function for this model. They played around with what variables to include in the model, using the correlation plot and their knowledge of baseball for some reference. The model they came up with is,  

&nbsp;

\begin{center}  

$glm(cy\_young \sim . - Tm - Lg - Year - GF - GS - IBB - BK - BF, data = train, family = "binomial")$

\end{center}  

&nbsp;

Now that they have decided on a model, they want to use a train/test split on the data to test the accuracy of their model. They create a for loop that splits the data randomly 25 different times. Each time through the loop, the model is used on the training data, predictions are made with the testing data, and the RMSE is then calculated. They then take the mean value of the 25 different RMSE values from their for loop. This is done to get a better idea of the true accuracy of their model. The find an RMSE value of `r rmse`, which they are happy with considering their limited time due to the urgency of the issue at hand. This is about a 5 percentage point error on their predictions of winning the Cy Young award. Since they are happy with the accuracy of the model, they now run it on their full data set to understand how each variable effects the probability of winning a Cy Young award. The coefficients from their model are,

```{r, include=TRUE, warning=FALSE, message=FALSE}

model$coefficients

```

Taking the exponent of the coefficient gives the expected increased odds on winning the Cy Young award. For example, an additional win is expected to multiply the odds of winning a Cy Young by $exp(0.4292769201) \approx 1.536$. The data scientists find some of these surprising, like the large positive coefficient on BB9. They want to look at the full summary of the results to better understand what is going on here.  

\pagebreak

```{r, include=TRUE, warning=FALSE, message=FALSE}

summary(model)

```

Just as they suspect, the coefficients that surprised them are not statistically significant. They now want to move their focus towards the high school pitchers and continue to work towards completing the task they were hired to do.  

\pagebreak

Satisfied with their model, they use it to predict the probability of winning the Cy Young award on the full data set of all the pitchers that pitched at least one inning from 1956 to 2022. They really like what they see with their predictions. The average predicted probability is `r avg_probs`, which is a good sign considering only a very tiny number of pitchers in this data have won the award. They look at the distribution of the predicted values to get a better understanding of what they are working with. They look at the overall picture and a more zoomed in picture that discludes probabilities below 0.005. The reason for this is there are so many really small values, it makes it impossible to see whats going on with any predicted values above 0.05.   

```{r, include=TRUE, warning=FALSE, message=FALSE, out.width=c('50%', '50%'), fig.show='hold'}

full_hist

zoomed_hist

```

They notice there are very few predicted probabilities that are more than 0.1. They filter the data to Cy Young winners with predicted probabilities of at least 0.1, and find `r nrow(above_10_and_won)` of the 124 Cy Young winners have a predicted probability of at least this value. They also find only `r nrow(above_10_and_lost)` of the `r nrow(model_data %>% filter(cy_young == 0))` non-winners have a predicted probability above 0.1. To see if this is a potential issue, they look further into these pitchers and found almost all of them received votes for the award in the respective year. Also, the pitchers that did not receive votes played on multiple teams that year, most resulting in splitting their time between the two leagues. This makes sense as to why they did not receive votes, because for most of the Cy Young award's history, the award is given to someone in both leagues. If your stats are split across the two leagues, you won't look as impressive when looking at the stats for either individual league. So, they really like 0.1 as a probability goal to set for the high school pitchers. They believe a predicted probability of at least 0.1 is a good value to say that pitcher is "Cy Young ready".  

\pagebreak  

\begingroup
\fontfamily{cmr}\fontsize{12}{12}\selectfont
**The Plan**
\endgroup

Now, they want to use the high schooler's stats with the model to get an idea of where they currently are in regards to being "Cy Young ready". The create predicted probabilities for each of the five pitchers.  

```{r, include=TRUE, warning=FALSE, message=FALSE}

high_schoolers[,c(1,33)]

```

They find there's definitely some work to be done to get them to the goal of a predicted probability of 0.1. Using the model's results and the correlation plot from before, they work to put together a four-year plan that will get at least the majority of the pitchers past the threshold. 

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{**The Plan**} \\\cline{1-5}
Stat & Year 1 & Year 2 & Year 3 & Year 4 \\\cline{1-5}
Wins & +1 & +1 & +2 & +2 \\\cline{1-5}
Losses & -1 & -1 & -1 & -1 \\\cline{1-5}
ERA & -5$\%$ & -10$\%$ & -15$\%$ & -20$\%$ \\\cline{1-5}
Hits & -5$\%$ & -5$\%$ & -5$\%$ & -10$\%$ \\\cline{1-5}
Runs & -5$\%$ & -5$\%$ & -5$\%$ & -10$\%$ \\\cline{1-5}
Earned Runs & -5$\%$ & -10$\%$ & -10$\%$ & -10$\%$ \\\cline{1-5}
Home Runs & -5$\%$ & -10$\%$ & -10$\%$ & -10$\%$ \\\cline{1-5}
Strike Outs & +5$\%$ & +10$\%$ & +10$\%$ & +10$\%$ \\\cline{1-5}
Walks & -5$\%$ & -10$\%$ & -5$\%$ & -10$\%$ \\\cline{1-5}
Hit Batters & -1 & -1 & -1 & -2 \\\cline{1-5}
Wild Pitches & -1 & -1 & -2 & -2 \\\cline{1-5}
\end{tabular}
\end{center}

&nbsp;

If Bob Lyur manages to stick to this plan and somehow improve each pitcher's stats accordingly, the data scientists are interested if this will be enough to get some of these pitchers above the goal they set. To see this, they rerun to model to see the predicted probabilities of the pitchers' chances of winning the Cy Young award. They get results with the pitchers' stats if Bob Lyur is able to successfully get each of them through the plan every year and make the set improvements.  

```{r, include=TRUE, warning=FALSE, message=FALSE}

high_schoolers_year_4_goals[,c(1,33)]

```

It's a success! The data scientists are happy with the results and eager to get the plan to Bob Lyur so he can begin putting it into action. Their job is done, and all that's left is to hope Bob Lyur can pull this off the save the reputation of the school!











